{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis Text Classifier",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1Ad7OpFbJv6RH0rVqYAR5oGcZ1UxomBfU",
      "authorship_tag": "ABX9TyOKiYfu6QBNF7XlZiv1gEWA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/awesome1021/awesome1021.github.io/blob/master/Sentiment_Analysis_Text_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbWN91SHdVt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn import datasets\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from google.colab import drive \n",
        "import random \n",
        "import numpy as np\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except: Exception\n",
        "pass\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "import pandas as pd\n",
        "\n",
        "# Reading data\n",
        "sentiment_data_path = '/content/drive/My Drive/Colab Notebooks/sentiment_text_data.csv'\n",
        "threat_data_url = 'https://raw.githubusercontent.com/t-davidson/hate-speech-and-offensive-language/master/data/labeled_data.csv'\n",
        "dfS = pd.read_csv(sentiment_data_path) # Sentiment classification\n",
        "dfT = pd.read_csv(threat_data_url) # Offensive statement classification \n",
        "dfS.columns = ['label', 'text']\n",
        "dfS.to_csv('sentiment.csv', index = False)\n",
        "dfS = pd.read_csv('sentiment.csv')\n",
        "\n",
        "# Cleaning up sentiment database\n",
        "dfS.loc[dfS.label == 4, 'label'] = 1\n",
        "for i, row in dfS.iterrows():\n",
        "  dfS.at[i, 'text'] = \" \".join(filter(lambda x:x[0]!='@', dfS.at[i, 'text'].split()))\n",
        "\n",
        "# Splitting the databases into 75% for training data and 25% for test data\n",
        "XS = dfS['text']\n",
        "YS = dfS['label']\n",
        "XS_train, XS_test, YS_train, YS_test = train_test_split(XS, YS, test_size = 0.25)\n",
        "XT = dfT['tweet']\n",
        "YT = dfT['class']\n",
        "XT_train, XT_test, YT_train, YT_test = train_test_split(XT, YT, test_size = 0.25)\n",
        "\n",
        "# Converting into TensorFlow datasets\n",
        "sentiment_train_data = tf.data.Dataset.from_tensor_slices((XS_train.values, YS_train.values))\n",
        "sentiment_test_data = tf.data.Dataset.from_tensor_slices((XS_test.values, YS_test.values))\n",
        "threat_train_data = tf.data.Dataset.from_tensor_slices((XT_train.values, YT_train.values))\n",
        "threat_test_data = tf.data.Dataset.from_tensor_slices((XT_test.values, YT_test.values))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OA98LdzH2o7F",
        "colab_type": "code",
        "outputId": "63344ceb-0ffd-4453-897d-5801be328147",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "# Training the data for sentiment\n",
        "embedding = \"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1\"\n",
        "hub_layer = hub.KerasLayer(embedding, input_shape=[], dtype = tf.string, trainable = True)\n",
        "modelS = tf.keras.Sequential()\n",
        "modelS.add(hub_layer)\n",
        "modelS.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "modelS.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "modelS.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "modelS.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "modelS.summary()\n",
        "\n",
        "train_test_size = len(XS_train)\n",
        "batch_size = 10\n",
        "train_tfds = sentiment_train_data.shuffle(train_test_size).batch(batch_size)\n",
        "test_tfds = sentiment_test_data.shuffle(train_test_size).batch(batch_size)\n",
        "history = modelS.fit(train_tfds, epochs=2, validation_data=test_tfds, verbose=1)\n",
        "\n",
        "results = modelS.evaluate(test_tfds, verbose=2)\n",
        "for name, value in zip(modelS.metrics_names, results):\n",
        "  print(\"%s: %.3f\" % (name, value)) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "keras_layer (KerasLayer)     (None, 128)               124642688 \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 16)                2064      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 124,645,041\n",
            "Trainable params: 124,645,041\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/2\n",
            "600/600 [==============================] - 22s 37ms/step - loss: 0.5888 - accuracy: 0.6876 - val_loss: 0.5387 - val_accuracy: 0.7200\n",
            "Epoch 2/2\n",
            "600/600 [==============================] - 22s 37ms/step - loss: 0.3180 - accuracy: 0.8651 - val_loss: 0.6167 - val_accuracy: 0.7150\n",
            "200/200 - 1s - loss: 0.6167 - accuracy: 0.7150\n",
            "loss: 0.617\n",
            "accuracy: 0.715\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMAlafGkK2Rl",
        "colab_type": "code",
        "outputId": "b1789327-d1f0-483f-95a9-1d052adfad8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "# Training the data for threats\n",
        "embedding = \"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1\"\n",
        "hub_layer = hub.KerasLayer(embedding, input_shape=[], dtype = tf.string, trainable = True)\n",
        "modelT = tf.keras.Sequential()\n",
        "modelT.add(hub_layer)\n",
        "modelT.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "modelT.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "modelT.add(tf.keras.layers.Dropout(0.2))\n",
        "modelT.add(tf.keras.layers.Dense(3, activation=tf.nn.softmax))\n",
        "modelT.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "modelT.summary()\n",
        "\n",
        "train_test_size = len(XT_train)\n",
        "batch_size = 10\n",
        "train_tfds = threat_train_data.shuffle(train_test_size).batch(batch_size)\n",
        "test_tfds = threat_test_data.shuffle(train_test_size).batch(batch_size)\n",
        "history = modelT.fit(train_tfds, epochs=3, validation_data=test_tfds, verbose=1)\n",
        "\n",
        "results = modelT.evaluate(test_tfds, verbose=2)\n",
        "for name, value in zip(modelT.metrics_names, results):\n",
        "  print(\"%s: %.3f\" % (name, value))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "keras_layer_1 (KerasLayer)   (None, 128)               124642688 \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 16)                2064      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 124,645,075\n",
            "Trainable params: 124,645,075\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "1859/1859 [==============================] - 68s 37ms/step - loss: 0.4663 - accuracy: 0.8341 - val_loss: 0.3549 - val_accuracy: 0.8689\n",
            "Epoch 2/3\n",
            "1859/1859 [==============================] - 68s 37ms/step - loss: 0.2344 - accuracy: 0.9185 - val_loss: 0.3931 - val_accuracy: 0.8628\n",
            "Epoch 3/3\n",
            "1859/1859 [==============================] - 68s 37ms/step - loss: 0.1319 - accuracy: 0.9528 - val_loss: 0.5353 - val_accuracy: 0.8565\n",
            "620/620 - 3s - loss: 0.5357 - accuracy: 0.8565\n",
            "loss: 0.536\n",
            "accuracy: 0.857\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOkoHpEGF36a",
        "colab_type": "code",
        "outputId": "ed684117-48bb-4bcd-df25-965d2010ace4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Check for the effectiveness of sentiment classification\n",
        "def check_sentiment(value):\n",
        "  if value == 0:\n",
        "    print(\"Likely negative sentiment (anger, sadness, etc.)\")\n",
        "  if value == 1:\n",
        "    print(\"Likely positive sentiment (happiness, excitement, etc.)\")\n",
        "\n",
        "index = random.randint(0, len(dfS) - 1)\n",
        "string = dfS['text'].values[index]\n",
        "print(string)\n",
        "prediction = modelS.predict_classes([string], batch_size=10)\n",
        "check_sentiment(prediction)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "still procrastinating... i hate organizing my clothes there's just so much....\n",
            "Likely negative sentiment (anger, sadness, etc.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3-iTFU5yoAj",
        "colab_type": "code",
        "outputId": "62fe9de8-939f-4c55-c54b-ab8d37c126f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Checking the effectiveness of threat classification\n",
        "def check_threat(value):\n",
        "  if value == 0:\n",
        "    print(\"Likely hate speech or threatening language\")\n",
        "  if value == 1:\n",
        "    print(\"Potentially threatening language\")\n",
        "  if value == 2:\n",
        "    print(\"Likely non-threatening language\")\n",
        "\n",
        "index = random.randint(0, len(dfT) - 1)\n",
        "string = dfT['tweet'].values[index]\n",
        "print(string)\n",
        "prediction = modelT.predict_classes([string], batch_size = 5)\n",
        "check_threat(prediction)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "&#8220;@NiggaImTatted: Cowboys win games during the regular season but when they get to the playoffs that trash af lol&#8221; hating ass nigga\n",
            "Likely hate speech or threatening language\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}